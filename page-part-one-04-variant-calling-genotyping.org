#+Title: Variant calling and genotyping - Complete workflow
#+Summary: Variant calling and genotyping
#+URL: part-one-04-variant-calling-genotyping.html
#+Save_as: part-one-04-variant-calling-genotyping.html
#+Status: hidden
#+OPTIONS: toc:3 num:nil html-postamble:nil

* Variant calling and genotyping

@@html:<div class="navLink">@@[[file:part-one-03-de-novo-assembly.html][Previous: De novo assembly]]@@html:</div>@@

** Mapping back the reads to the consensus sequences

*** TODO Examine Phred score profiles

Now that we have the consensus sequences, we need to map back each read to them
to align matching reads together and detect polymorphic positions.

Read calling during sequencing is not perfect, and for Illumina the miscall
rate is around 1% (Nielsen et al. 2011). To minimise the errors during SNP
detection (false positives), we should process the reads to remove bases with
low quality score from the analysis.

Let's examine the distribution of quality scores for each set of reads. We can
use a homemade [[file:resources/extract_phred_profiles.py][python script]] (=extract_phred_profiles.py=) to get the Phred
score in a tabular format. We can then use R to plot the average profiles.

#+BEGIN_SRC bash
python extract_phred_profiles.py s_6_2_sequence.fastq
#+END_SRC

#+BEGIN_SRC R
# R script
#
# Plot the Phred profile for a FASTQ file
#----------------------------------------

p = read.table("s_6_2_sequence.fastq.phred_profile", sep = "\t", header = F)
boxplot(p, outline = F)
#+END_SRC

How does the quality score change along the reads?

*** TODO Read trimming

To remove low quality bases from the 3' end, we will remove bases from this end
until the Phred quality score reached 20 (error rate = 1%). Remaining reads
with length less than 40 will be discarded.

We could do that using Fastq Quality trimmer as implemented in Galaxy tools,
but we can also use a homemade python script (=trim_reads.py=):

#+BEGIN_SRC bash
# Usage: python script.py inputFile phredThreshold lengthThreshold
python trim_reads.py s_6_2_sequence.fastq 20 40
#+END_SRC

We apply the quality trimming to each forward and reverse reads files for each
population:
#+BEGIN_SRC bash
ls -1
#+END_SRC
#+BEGIN_SRC 
ABB.pair.for.fastq.clean
ABB.pair.rev.fastq.clean
BYN.pair.for.fastq.clean
BYN.pair.rev.fastq.clean
HKI.pair.for.fastq.clean
HKI.pair.rev.fastq.clean
LEV.pair.for.fastq.clean
LEV.pair.rev.fastq.clean
POR.pair.for.fastq.clean
POR.pair.rev.fastq.clean
PYO.pair.for.fastq.clean
PYO.pair.rev.fastq.clean
RYT.pair.for.fastq.clean
RYT.pair.rev.fastq.clean
SKA.pair.for.fastq.clean
SKA.pair.rev.fastq.clean
#+END_SRC
#+BEGIN_SRC bash
python trim_reads.py *.fastq.clean 20 40
#+END_SRC

Also apply this trimming to the files containing all the forward reads and all
the reverse reads used for the /de novo/ assembly. How would you examine the
resulting distribution of read lengths in those trimmed files? If you have
time, do it.

** Variant calling

*** TODO Prepare input files for Maq

We will use the =Maq= program to perform the read alignment. A detailed
explanation of the Maq workflow is available [[http://maq.sourceforge.net/maq-man.shtml][here]].

We have the reference files:
#+BEGIN_SRC 
consensus-for.fasta
consensus-rev.fasta
#+END_SRC
and the reads files:
#+BEGIN_SRC 
ABB.pair.for.fastq.clean.trimmed
ABB.pair.rev.fastq.clean.trimmed
BYN.pair.for.fastq.clean.trimmed
BYN.pair.rev.fastq.clean.trimmed
HKI.pair.for.fastq.clean.trimmed
HKI.pair.rev.fastq.clean.trimmed
LEV.pair.for.fastq.clean.trimmed
LEV.pair.rev.fastq.clean.trimmed
POR.pair.for.fastq.clean.trimmed
POR.pair.rev.fastq.clean.trimmed
PYO.pair.for.fastq.clean.trimmed
PYO.pair.rev.fastq.clean.trimmed
RYT.pair.for.fastq.clean.trimmed
RYT.pair.rev.fastq.clean.trimmed
SKA.pair.for.fastq.clean.trimmed
SKA.pair.rev.fastq.clean.trimmed
#+END_SRC

First, we convert the fasta reference to Maq bfa format:
#+BEGIN_SRC bash
maq fasta2bfa consensus-for.fasta consensus-for.bfa
maq fasta2bfa consensus-rev.fasta consensus-rev.bfa
#+END_SRC

Then we convert the fastq read files to Maq bfq format using a small bash
script, =convert-trimmed-to-bfq.sh=. The script file is:
#+BEGIN_SRC
#!/bin/bash
FILES=`ls *.trimmed`   # get the list of trimmed read files

for f in $FILES
do
  echo $f
  maq sol2sanger $f $f.sanger       # convert Illumina 1.5+ to Sanger scores
  maq fastq2bfq $f.sanger $f.bfq    # convert fastq to bfq
done
#+END_SRC
We use it with:
#+BEGIN_SRC bash
bash convert-trimmed-to-bfq.sh
#+END_SRC

We also generate the bfq files for all the concatenated forward reads and all
the concatenated reverse reads:
#+BEGIN_SRC bash
# Concatenate the forward and reverse reads
cat *for*.trimmed > reads.for.trimmed
cat *rev*.trimmed > reads.rev.trimmed
# Convert to bfq
maq sol2sanger reads.for.trimmed reads.for.sanger
maq sol2sanger reads.rev.trimmed reads.rev.sanger
maq fastq2bfq reads.for.sanger reads.for.bfq
maq fastq2bfq reads.rev.sanger reads.rev.bfq
#+END_SRC

At this point, the fasta and fastq files have been converted to the binary
format used by Maq.

*** TODO Read alignment to the consensus sequences

Here we will use all the forward reads and all the reverse reads and align them
to the consensus sequences. For the practicals, we work only with the forward
reads from now on.

Again, this step could be long, so we submit it as a job in the file
=map-maq-for-job=. The map file is a binary file, but we also convert it to a
human-readable form with =mapview=:
#+BEGIN_SRC 
#!/bin/bash
#$ -S /bin/bash
#$ -M matthieu.bruneaux@ens-lyon.org
#$ -V
#$ -m bea
#$ -cwd
# Maq map forward reads
maq map for.map consensus-for.bfa reads.for.bfq
maq mapview for.map > for.map.txt
#+END_SRC

We submit the job with =qsub -q long.q map-maq-for-job=. Have a look at the map
with =less -S for.map.txt=. Using the [[http://maq.sourceforge.net/maq-manpage.shtml][man page]] (search for "mapview" with
CTRL+F), understand the output format, and in particular look for the number of
mismatches of the best hits. Can you identify which reads are similar to the
consensus sequence and which harbour potential SNPs?

*** TODO SNP calling

Now that the reads have been mapped back, we can perform the SNP calling
operation.

We use the =assemble= and the =cns2snp= function of Maq to extract the SNP
information. The job file is =maq-assemble-cns2snp-job=:
#+BEGIN_SRC 
#!/bin/bash
#$ -S /bin/bash
#$ -M matthieu.bruneaux@ens-lyon.org
#$ -V
#$ -m bea
#$ -cwd
# Maq assemble
maq assemble for.cns consensus-for.bfa for.map 2> for-assemble.log
# Maq cns2snp
maq cns2snp for.cns > for.snp
#+END_SRC

As usual, we submit the job with =qsub -q long.q
map-assemble-cns2snp-job=. Have a look at the output file =for.snp=. How many
putative SNPs were detected? The format of the output file is explained on the
[[http://maq.sourceforge.net/maq-manpage.shtml][man page]] (search for "cns2snp" with CTRL+F).

A very useful file for detailed SNP filtering is the pileup file. We can
produce it with:
#+BEGIN_SRC bash
maq pileup consensus-for.bfa for.map > for.pileup
#+END_SRC

Have a look at the pileup file with =less -S for.pileup=. Can you understand
its format? (see the [[http://maq.sourceforge.net/maq-manpage.shtml][man page]], search for "pileup") Can you identify the
contigs with low coverage and high coverage? Can you identify putative
polymorphic positions? Can you identify positions that look like genuine SNPs?

*** SNP filtering

The SNP filtering is a delicate operation, since it is important to make a
distinction between genuine SNPs and false positives due to sequencing
errors. In this case, we used the following criteria:
- alleles with a frequency equal to or less than the error rate (i.e. <=0.01)
  were removed
- retained SNP should be biallelic
- read depth (coverage) should be greater than 10 but less than 1000
- at least 3 reads should be observed per SNP allele
- maximum two SNPs per consensus sequence
- not falling less than 5 bases away from identified microsatellites (if
  microsatellites were kept)

Those criteria are rather on the conservative side and might result in losing
genuine SNPs. Explain why and propose better criteria.

Some SNP calling methods use more elaborate approaches such as maximum
likelihood and bayesian methods. See the *bibliography notes* section.

Since the SNP filtering as done here is mainly an exercise in scripting (using
R or Python for example), we will not perform it during the practicals and we
will use ready-made output files containing the SNP information.

** Diagnostic plots for reality check

How many SNPs do we have? What is the coverage per populations? Do we have to
pool some populations?

@@html:<div class="navLink">@@[[file:part-one-05-downstream-analysis.html][Next: Downstream analysis]]@@html:</div>@@
